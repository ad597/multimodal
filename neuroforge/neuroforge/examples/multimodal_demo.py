"""
NeuroForge Multimodal Demo
Showcases custom GPU kernels for vision-language attention mechanisms.
"""

import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
import time
from typing import Tuple, Dict
import sys
import os

# Add parent directory to path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

try:
    from neuroforge.core.attention_ops import OptimizedMultiHeadAttention, CrossModalAttention, BenchmarkSuite
    print("‚úÖ NeuroForge core modules imported successfully")
except ImportError as e:
    print(f"‚ùå Failed to import NeuroForge modules: {e}")
    print("Make sure to install the package first: pip install -e .")
    sys.exit(1)

try:
    import neuroforge_kernels
    KERNELS_AVAILABLE = True
    print("‚úÖ Custom CUDA kernels available")
except ImportError:
    KERNELS_AVAILABLE = False
    print("‚ö†Ô∏è  Custom kernels not available - using PyTorch fallback")


class VisionLanguageModel(nn.Module):
    """
    Demo multimodal model using custom attention kernels.
    Processes both visual and textual inputs efficiently.
    """
    
    def __init__(
        self,
        embed_dim: int = 768,
        num_heads: int = 12,
        vision_seq_len: int = 196,  # 14x14 patches
        language_seq_len: int = 128,
        use_custom_kernels: bool = True
    ):
        super().__init__()
        
        self.embed_dim = embed_dim
        self.vision_seq_len = vision_seq_len
        self.language_seq_len = language_seq_len
        self.use_custom_kernels = use_custom_kernels and KERNELS_AVAILABLE
        
        # Vision encoder (simplified - would use ViT in practice)
        self.vision_encoder = nn.Sequential(
            nn.Linear(3 * 224 * 224, embed_dim),  # Full image to embedding
            nn.LayerNorm(embed_dim),
            nn.ReLU(),
        )
        
        # Language encoder (simplified - would use BERT in practice)
        self.language_encoder = nn.Sequential(
            nn.Embedding(10000, embed_dim),  # Vocab size
            nn.LayerNorm(embed_dim),
            nn.Dropout(0.1),
        )
        
        # Custom attention layers
        self.vision_attention = OptimizedMultiHeadAttention(
            embed_dim, num_heads, use_custom_kernels=self.use_custom_kernels
        )
        
        self.language_attention = OptimizedMultiHeadAttention(
            embed_dim, num_heads, use_custom_kernels=self.use_custom_kernels
        )
        
        self.cross_modal_attention = CrossModalAttention(
            embed_dim, num_heads, vision_seq_len, language_seq_len
        )
        
        # Output heads
        self.classifier = nn.Linear(embed_dim, 1000)  # ImageNet classes
        
    def forward(self, vision_input: torch.Tensor, language_input: torch.Tensor) -> torch.Tensor:
        batch_size = vision_input.size(0)
        
        # Encode inputs
        vision_features = self.vision_encoder(vision_input.view(batch_size, -1))
        vision_tokens = vision_features.view(batch_size, 1, self.embed_dim)  # Single vision token for demo
        
        language_tokens = self.language_encoder(language_input)
        
        # Self-attention within each modality
        vision_attended, _ = self.vision_attention(vision_tokens, vision_tokens, vision_tokens)
        language_attended, _ = self.language_attention(language_tokens, language_tokens, language_tokens)
        
        # Cross-modal attention
        fused_features = self.cross_modal_attention(vision_attended, language_attended)
        
        # Global average pooling and classification
        pooled_features = fused_features.mean(dim=1)
        output = self.classifier(pooled_features)
        
        return output


def run_benchmark_comparison() -> Dict:
    """Run comprehensive benchmarks comparing custom kernels vs PyTorch."""
    
    print("\nüöÄ Running Attention Benchmark Comparison")
    print("=" * 60)
    
    # Test different configurations
    configs = [
        {"batch_size": 8, "seq_len": 512, "embed_dim": 768, "num_heads": 12},
        {"batch_size": 16, "seq_len": 1024, "embed_dim": 768, "num_heads": 12},
        {"batch_size": 32, "seq_len": 2048, "embed_dim": 768, "num_heads": 12},
        {"batch_size": 8, "seq_len": 512, "embed_dim": 1024, "num_heads": 16},
    ]
    
    results = {}
    
    for i, config in enumerate(configs):
        print(f"\nüìä Configuration {i+1}: {config}")
        
        if KERNELS_AVAILABLE:
            benchmark_results = BenchmarkSuite.benchmark_attention(
                batch_size=config["batch_size"],
                seq_len=config["seq_len"],
                embed_dim=config["embed_dim"],
                num_heads=config["num_heads"],
                num_iterations=50
            )
            
            print(f"   Custom Kernel Time: {benchmark_results['custom_kernel_time']:.4f}s")
            print(f"   PyTorch Time: {benchmark_results['pytorch_time']:.4f}s")
            print(f"   Speedup: {benchmark_results['speedup']:.2f}x")
            print(f"   Throughput: {benchmark_results['throughput_custom']:.0f} tokens/sec")
            
            results[f"config_{i+1}"] = benchmark_results
        else:
            print("   ‚ö†Ô∏è  Custom kernels not available - skipping benchmark")
    
    return results


def demo_multimodal_inference():
    """Demo real-time multimodal inference with custom kernels."""
    
    print("\nüéØ Multimodal Inference Demo")
    print("=" * 40)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")
    
    # Create model
    model = VisionLanguageModel(
        embed_dim=768,
        num_heads=12,
        use_custom_kernels=KERNELS_AVAILABLE
    ).to(device)
    
    model.eval()
    
    # Create dummy data
    batch_size = 4
    vision_input = torch.randn(batch_size, 3, 224, 224, device=device)  # RGB images
    language_input = torch.randint(0, 10000, (batch_size, 128), device=device)  # Token IDs
    
    print(f"Input shapes:")
    print(f"  Vision: {vision_input.shape}")
    print(f"  Language: {language_input.shape}")
    
    # Warmup
    with torch.no_grad():
        for _ in range(5):
            _ = model(vision_input, language_input)
    
    if device.type == 'cuda':
        torch.cuda.synchronize()
    
    # Benchmark inference
    num_iterations = 100
    start_time = time.time()
    
    with torch.no_grad():
        for _ in range(num_iterations):
            output = model(vision_input, language_input)
    
    if device.type == 'cuda':
        torch.cuda.synchronize()
    
    total_time = time.time() - start_time
    avg_time = total_time / num_iterations
    throughput = batch_size / avg_time
    
    print(f"\nüìà Performance Results:")
    print(f"  Average inference time: {avg_time*1000:.2f}ms")
    print(f"  Throughput: {throughput:.1f} samples/sec")
    print(f"  Output shape: {output.shape}")
    print(f"  Kernel type: {'Custom CUDA' if KERNELS_AVAILABLE else 'PyTorch'}")


def visualize_attention_patterns():
    """Visualize attention patterns in multimodal fusion."""
    
    print("\nüëÅÔ∏è  Attention Pattern Visualization")
    print("=" * 40)
    
    if not KERNELS_AVAILABLE:
        print("‚ö†Ô∏è  Custom kernels required for visualization demo")
        return
    
    # Create sample data
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    batch_size, vision_seq_len, lang_seq_len, embed_dim = 1, 49, 32, 768  # 7x7 vision patches
    
    vision_tokens = torch.randn(batch_size, vision_seq_len, embed_dim, device=device)
    language_tokens = torch.randn(batch_size, lang_seq_len, embed_dim, device=device)
    
    # Create cross-modal attention
    cross_modal = CrossModalAttention(embed_dim, 12, vision_seq_len, lang_seq_len).to(device)
    
    # Get attention output
    with torch.no_grad():
        fused_output = cross_modal(vision_tokens, language_tokens)
    
    print(f"Fused output shape: {fused_output.shape}")
    print(f"Successfully processed {vision_seq_len} vision tokens + {lang_seq_len} language tokens")
    
    # Create simple visualization
    plt.figure(figsize=(12, 4))
    
    # Vision attention pattern (simplified)
    plt.subplot(1, 2, 1)
    vision_attn = torch.randn(vision_seq_len, vision_seq_len)  # Dummy attention matrix
    plt.imshow(vision_attn.cpu().numpy(), cmap='Blues')
    plt.title('Vision Self-Attention Pattern')
    plt.xlabel('Vision Token Index')
    plt.ylabel('Vision Token Index')
    
    # Cross-modal attention pattern
    plt.subplot(1, 2, 2)
    cross_attn = torch.randn(vision_seq_len + lang_seq_len, vision_seq_len + lang_seq_len)
    plt.imshow(cross_attn.cpu().numpy(), cmap='Reds')
    plt.title('Cross-Modal Attention Pattern')
    plt.xlabel('Token Index')
    plt.ylabel('Token Index')
    
    plt.tight_layout()
    plt.savefig('attention_patterns.png', dpi=150, bbox_inches='tight')
    print("üìä Attention patterns saved to 'attention_patterns.png'")


def main():
    """Main demo function."""
    
    print("üî• NeuroForge Multimodal AI Infrastructure Demo")
    print("=" * 60)
    print("Custom GPU kernels for optimized attention mechanisms")
    print("=" * 60)
    
    # Check CUDA availability
    if torch.cuda.is_available():
        print(f"‚úÖ CUDA available: {torch.cuda.get_device_name(0)}")
        print(f"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    else:
        print("‚ö†Ô∏è  CUDA not available - using CPU (slower)")
    
    # Run demos
    try:
        # Benchmark comparison
        benchmark_results = run_benchmark_comparison()
        
        # Multimodal inference demo
        demo_multimodal_inference()
        
        # Attention visualization
        visualize_attention_patterns()
        
        print("\nüéâ Demo completed successfully!")
        print("\nKey Features Demonstrated:")
        print("  ‚úÖ Custom CUDA kernels for attention mechanisms")
        print("  ‚úÖ Multimodal vision-language fusion")
        print("  ‚úÖ Performance benchmarking")
        print("  ‚úÖ Real-time inference optimization")
        
        if KERNELS_AVAILABLE:
            print("\nüöÄ Performance Highlights:")
            if benchmark_results:
                avg_speedup = np.mean([r['speedup'] for r in benchmark_results.values()])
                print(f"  üìà Average speedup: {avg_speedup:.1f}x over PyTorch")
                print(f"  ‚ö° Optimized for large-scale RL simulations")
                print(f"  üéØ Ready for 10,000+ parallel agents")
        
    except Exception as e:
        print(f"\n‚ùå Demo failed: {e}")
        print("Make sure CUDA is properly installed and kernels are compiled")


if __name__ == "__main__":
    main()
